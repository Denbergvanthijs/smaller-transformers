{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from torch import nn\n",
    "from transformers import pipeline\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download and extract the [2018 Wikipedia dumps](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2735) in the data folder for the 15 XNLI languages :\n",
    "\n",
    "```bash\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/en.txt.gz -P data\n",
    "gunzip data/en.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/fr.txt.gz -P data\n",
    "gunzip data/fr.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/es.txt.gz -P data\n",
    "gunzip data/es.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/de.txt.gz -P data\n",
    "gunzip data/de.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/zh.txt.gz -P data\n",
    "gunzip data/zh.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/ar.txt.gz -P data\n",
    "gunzip data/ar.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/ru.txt.gz -P data\n",
    "gunzip data/ru.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/vi.txt.gz -P data\n",
    "gunzip data/vi.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/el.txt.gz -P data\n",
    "gunzip data/el.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/bg.txt.gz -P data\n",
    "gunzip data/bg.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/th.txt.gz -P data\n",
    "gunzip data/th.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/tr.txt.gz -P data\n",
    "gunzip data/tr.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/hi.txt.gz -P data\n",
    "gunzip data/hi.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/ur.txt.gz -P data\n",
    "gunzip data/ur.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/sw.txt.gz -P data\n",
    "gunzip data/sw.txt.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_vocab = list(tokenizer.vocab.keys())\n",
    "len(bert_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40331979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85988806"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines = 0\n",
    "num_long_lines = 0\n",
    "path_english = 'data/en.txt'\n",
    "\n",
    "with open(path_english) as infile:\n",
    "    for line in infile:\n",
    "        num_lines += 1\n",
    "        if len(line)>5:\n",
    "            num_long_lines += 1\n",
    "print(num_long_lines)\n",
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 619.2274067401886\n",
      "2.0 1110.1034936904907\n",
      "3.0 1541.703180551529\n",
      "4.0 1903.5885076522827\n",
      "5.0 2333.342573404312\n",
      "6.0 2735.3478829860687\n",
      "7.0 3114.140008687973\n",
      "8.0 3427.5012040138245\n",
      "9.0 3733.960935115814\n",
      "10.0 4035.6765863895416\n",
      "11.0 4335.431081771851\n",
      "12.0 4630.971633672714\n",
      "13.0 4921.836084842682\n",
      "14.0 5209.920123338699\n",
      "15.0 5493.038271903992\n",
      "16.0 5775.249083280563\n",
      "17.0 6066.010138750076\n",
      "18.0 6382.9865119457245\n",
      "19.0 6653.678861618042\n",
      "20.0 6921.082093477249\n",
      "21.0 7185.778695106506\n",
      "22.0 7446.724161863327\n",
      "23.0 7717.169724225998\n",
      "24.0 7989.366755723953\n",
      "25.0 8292.291065216064\n",
      "26.0 10190.70014500618\n",
      "27.0 10436.101276874542\n",
      "28.0 10673.275538444519\n",
      "29.0 10905.899139642715\n",
      "30.0 11140.80565571785\n",
      "31.0 11374.124992609024\n",
      "32.0 11606.152292251587\n",
      "33.0 11839.679318904877\n",
      "34.0 12076.439000844955\n",
      "35.0 12306.209696769714\n",
      "36.0 12536.870078802109\n",
      "37.0 12760.052688360214\n",
      "38.0 12981.1307888031\n",
      "39.0 13195.619985103607\n",
      "40.0 13406.025039196014\n",
      "41.0 13623.775834798813\n",
      "42.0 13849.890589237213\n",
      "43.0 14055.302285909653\n",
      "44.0 14256.839665412903\n",
      "45.0 14476.429979801178\n",
      "46.0 14691.234797239304\n",
      "47.0 14899.632244110107\n",
      "48.0 15116.59131026268\n",
      "49.0 15336.598145008087\n",
      "50.0 15556.692955732346\n",
      "51.0 15759.36065530777\n",
      "52.0 15975.044377326965\n",
      "53.0 16191.90760922432\n",
      "54.0 16408.929543733597\n",
      "55.0 16634.309510707855\n",
      "56.0 16851.274728298187\n",
      "57.0 17067.379917383194\n",
      "58.0 17285.34434723854\n",
      "59.0 17501.176157474518\n",
      "60.0 17720.637830495834\n",
      "61.0 17950.966504573822\n",
      "62.0 18177.637378931046\n",
      "63.0 18398.820682764053\n",
      "64.0 18615.171079158783\n",
      "65.0 18837.80086374283\n",
      "66.0 19061.358541965485\n",
      "67.0 19270.73300719261\n",
      "68.0 19481.61249780655\n",
      "69.0 19698.391099214554\n",
      "70.0 19920.02815389633\n",
      "71.0 20136.883369207382\n",
      "72.0 20352.27623128891\n",
      "73.0 20568.078628063202\n",
      "74.0 20782.368901014328\n",
      "75.0 20987.054142951965\n",
      "76.0 21200.75761938095\n",
      "77.0 21416.277082681656\n",
      "78.0 21627.060205698013\n",
      "79.0 21841.209245681763\n",
      "80.0 22054.381521224976\n",
      "81.0 22266.503155469894\n",
      "82.0 22472.750094413757\n",
      "83.0 22678.290512800217\n",
      "84.0 22883.468999147415\n",
      "85.0 23083.919184446335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_english = 'data/en.txt'\n",
    "english_tokens = dict()\n",
    "english_tokens_unique = dict()\n",
    "\n",
    "cpt = 0\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "with open(path_english) as infile:\n",
    "    for line in infile:\n",
    "        cpt += 1\n",
    "        if cpt%1000000==0:\n",
    "            print(cpt/1000000, time.time()-t0)\n",
    "        if len(line)>5:\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            for token in tokens:\n",
    "                if token not in english_tokens:\n",
    "                    english_tokens[token] = 1\n",
    "                else:\n",
    "                    english_tokens[token] += 1\n",
    "            for token in list(set(tokens)):\n",
    "                if token not in english_tokens_unique:\n",
    "                    english_tokens_unique[token] = 1\n",
    "                else:\n",
    "                    english_tokens_unique[token] += 1\n",
    "len(english_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/english_freqs_lines.json', 'w') as outfile:\n",
    "    json.dump(english_tokens_unique, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2016"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seuil_en = int(num_long_lines*0.005/100)\n",
    "seuil_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28458"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(english_tokens_unique))\n",
    "\n",
    "selected_english_tokens = []\n",
    "\n",
    "for tok in english_tokens_unique:\n",
    "    if english_tokens_unique[tok] >= seuil_en:\n",
    "        selected_english_tokens.append(tok)\n",
    "len(selected_english_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14157922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30963413"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines = 0\n",
    "num_long_lines = 0\n",
    "path_french = 'data/fr.txt'\n",
    "\n",
    "with open(path_french) as infile:\n",
    "    for line in infile:\n",
    "        num_lines += 1\n",
    "        if len(line)>5:\n",
    "            num_long_lines += 1\n",
    "print(num_long_lines)\n",
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 39810.99858593941\n",
      "2.0 40137.153487443924\n",
      "3.0 40431.46473360062\n",
      "4.0 40708.45555782318\n",
      "5.0 40973.78323554993\n",
      "6.0 41215.0137860775\n",
      "7.0 41478.401816129684\n",
      "8.0 41750.72194433212\n",
      "9.0 42008.57228732109\n",
      "10.0 42283.9727435112\n",
      "11.0 42519.331260204315\n",
      "12.0 42731.96723771095\n",
      "13.0 42995.029457330704\n",
      "14.0 43226.48523283005\n",
      "15.0 43466.27327799797\n",
      "16.0 43696.58191180229\n",
      "17.0 43921.311435222626\n",
      "18.0 44130.201075553894\n",
      "19.0 44332.168563365936\n",
      "20.0 44539.21525597572\n",
      "21.0 44742.61037325859\n",
      "22.0 44933.7055413723\n",
      "23.0 45125.2714009285\n",
      "24.0 45318.316663980484\n",
      "25.0 45513.25025177002\n",
      "26.0 45709.20458507538\n",
      "27.0 45917.36214637756\n",
      "28.0 46108.93855166435\n",
      "29.0 46304.16796207428\n",
      "30.0 46499.237392902374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79453"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_french = 'data/fr.txt'\n",
    "french_tokens = dict()\n",
    "french_tokens_unique = dict()\n",
    "cpt = 0\n",
    "\n",
    "with open(path_french) as infile:\n",
    "    for line in infile:\n",
    "        cpt += 1\n",
    "        if cpt%1000000==0:\n",
    "            print(cpt/1000000, time.time()-t0)\n",
    "        if len(line)>5:\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            for token in tokens:\n",
    "                if token not in french_tokens:\n",
    "                    french_tokens[token] = 1\n",
    "                else:\n",
    "                    french_tokens[token] += 1\n",
    "            for token in list(set(tokens)):\n",
    "                if token not in french_tokens_unique:\n",
    "                    french_tokens_unique[token] = 1\n",
    "                else:\n",
    "                    french_tokens_unique[token] += 1\n",
    "len(french_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/french_freqs_lines.json', 'w') as outfile:\n",
    "    json.dump(french_tokens_unique, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "707"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seuil_fr = int(num_long_lines*0.005/100)\n",
    "seuil_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24482"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(french_tokens_unique))\n",
    "\n",
    "selected_french_tokens = []\n",
    "\n",
    "for tok in french_tokens_unique:\n",
    "    if french_tokens_unique[tok] >= seuil_fr:\n",
    "        selected_french_tokens.append(tok)\n",
    "len(selected_french_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10366564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21981550"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines = 0\n",
    "num_long_lines = 0\n",
    "path_spanish = 'data/es.txt'\n",
    "\n",
    "with open(path_spanish) as infile:\n",
    "    for line in infile:\n",
    "        num_lines += 1\n",
    "        if len(line)>5:\n",
    "            num_long_lines += 1\n",
    "print(num_long_lines)\n",
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 47031.93489098549\n",
      "2.0 47351.575922489166\n",
      "3.0 47646.15384101868\n",
      "4.0 47929.57264137268\n",
      "5.0 48202.53243994713\n",
      "6.0 48464.20136833191\n",
      "7.0 48718.98474192619\n",
      "8.0 48971.43407225609\n",
      "9.0 49217.07389807701\n",
      "10.0 49461.11621928215\n",
      "11.0 49698.869629621506\n",
      "12.0 49923.902220487595\n",
      "13.0 50120.99433493614\n",
      "14.0 50328.89301943779\n",
      "15.0 50556.55177426338\n",
      "16.0 50828.06156396866\n",
      "17.0 51066.96748971939\n",
      "18.0 51323.644045352936\n",
      "19.0 51575.98098778725\n",
      "20.0 51824.440475702286\n",
      "21.0 52069.99340748787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80150"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_spanish = 'data/es.txt'\n",
    "spanish_tokens = dict()\n",
    "spanish_tokens_unique = dict()\n",
    "cpt = 0\n",
    "\n",
    "with open(path_spanish) as infile:\n",
    "    for line in infile:\n",
    "        cpt += 1\n",
    "        if cpt%1000000==0:\n",
    "            print(cpt/1000000, time.time()-t0)\n",
    "        if len(line)>5:\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            for token in tokens:\n",
    "                if token not in spanish_tokens:\n",
    "                    spanish_tokens[token] = 1\n",
    "                else:\n",
    "                    spanish_tokens[token] += 1\n",
    "            for token in list(set(tokens)):\n",
    "                if token not in spanish_tokens_unique:\n",
    "                    spanish_tokens_unique[token] = 1\n",
    "                else:\n",
    "                    spanish_tokens_unique[token] += 1\n",
    "len(spanish_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/spanish_freqs_lines.json', 'w') as outfile:\n",
    "    json.dump(spanish_tokens_unique, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80150"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spanish_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "518"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seuil_es = int(num_long_lines*0.005/100)\n",
    "seuil_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26346"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(spanish_tokens_unique))\n",
    "\n",
    "selected_spanish_tokens = []\n",
    "\n",
    "for tok in spanish_tokens_unique:\n",
    "    if spanish_tokens_unique[tok] >= seuil_es:\n",
    "        selected_spanish_tokens.append(tok)\n",
    "len(selected_spanish_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16089720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34715825"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines = 0\n",
    "num_long_lines = 0\n",
    "path_german = 'data/de.txt'\n",
    "\n",
    "with open(path_german) as infile:\n",
    "    for line in infile:\n",
    "        num_lines += 1\n",
    "        if len(line)>5:\n",
    "            num_long_lines += 1\n",
    "print(num_long_lines)\n",
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 54116.66038155556\n",
      "2.0 54577.04998540878\n",
      "3.0 54942.04729270935\n",
      "4.0 55246.97596168518\n",
      "5.0 55532.67652797699\n",
      "6.0 55822.38264584541\n",
      "7.0 56106.992736816406\n",
      "8.0 56388.41462779045\n",
      "9.0 56674.296174287796\n",
      "10.0 56943.6097369194\n",
      "11.0 57210.43217921257\n",
      "12.0 57457.73711562157\n",
      "13.0 57720.30193734169\n",
      "14.0 57982.308339595795\n",
      "15.0 58241.14898991585\n",
      "16.0 58501.795637369156\n",
      "17.0 58760.0890789032\n",
      "18.0 59018.77443575859\n",
      "19.0 59278.861579179764\n",
      "20.0 59535.25803351402\n",
      "21.0 59790.468202352524\n",
      "22.0 60043.414365291595\n",
      "23.0 60314.525517463684\n",
      "24.0 60575.48369884491\n",
      "25.0 60859.920449495316\n",
      "26.0 61181.619931936264\n",
      "27.0 61499.977361917496\n",
      "28.0 61816.68152093887\n",
      "29.0 62186.0216858387\n",
      "30.0 62483.2623193264\n",
      "31.0 62740.58487343788\n",
      "32.0 62993.88537359238\n",
      "33.0 63248.73959302902\n",
      "34.0 63495.93461251259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78478"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_german = 'data/de.txt'\n",
    "german_tokens = dict()\n",
    "german_tokens_unique = dict()\n",
    "cpt = 0\n",
    "\n",
    "with open(path_german) as infile:\n",
    "    for line in infile:\n",
    "        cpt += 1\n",
    "        if cpt%1000000==0:\n",
    "            print(cpt/1000000, time.time()-t0)\n",
    "        if len(line)>5:\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            for token in tokens:\n",
    "                if token not in german_tokens:\n",
    "                    german_tokens[token] = 1\n",
    "                else:\n",
    "                    german_tokens[token] += 1\n",
    "            for token in list(set(tokens)):\n",
    "                if token not in german_tokens_unique:\n",
    "                    german_tokens_unique[token] = 1\n",
    "                else:\n",
    "                    german_tokens_unique[token] += 1\n",
    "len(german_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/german_freqs_lines.json', 'w') as outfile:\n",
    "    json.dump(german_tokens_unique, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seuil_de = int(num_long_lines*0.005/100)\n",
    "seuil_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26031"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(german_tokens_unique))\n",
    "\n",
    "selected_german_tokens = []\n",
    "\n",
    "for tok in german_tokens_unique:\n",
    "    if german_tokens_unique[tok] >= seuil_de:\n",
    "        selected_german_tokens.append(tok)\n",
    "len(selected_german_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4487824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10878878"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines = 0\n",
    "num_long_lines = 0\n",
    "path_chinese = 'data/zh.txt'\n",
    "\n",
    "with open(path_chinese) as infile:\n",
    "    for line in infile:\n",
    "        num_lines += 1\n",
    "        if len(line)>5:\n",
    "            num_long_lines += 1\n",
    "print(num_long_lines)\n",
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 63904.307149887085\n",
      "2.0 64103.00419545174\n",
      "3.0 64302.74151086807\n",
      "4.0 64489.303250312805\n",
      "5.0 64671.73628425598\n",
      "6.0 64847.511660814285\n",
      "7.0 64970.82591891289\n",
      "8.0 65122.165700912476\n",
      "9.0 65302.25393438339\n",
      "10.0 65474.06551837921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65060"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_tokens = dict()\n",
    "chinese_tokens_unique = dict()\n",
    "cpt = 0\n",
    "\n",
    "with open(path_chinese) as infile:\n",
    "    for line in infile:\n",
    "        cpt += 1\n",
    "        if cpt%1000000==0:\n",
    "            print(cpt/1000000, time.time()-t0)\n",
    "        if len(line)>5:\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            for token in tokens:\n",
    "                if token not in chinese_tokens:\n",
    "                    chinese_tokens[token] = 1\n",
    "                else:\n",
    "                    chinese_tokens[token] += 1\n",
    "            for token in list(set(tokens)):\n",
    "                if token not in chinese_tokens_unique:\n",
    "                    chinese_tokens_unique[token] = 1\n",
    "                else:\n",
    "                    chinese_tokens_unique[token] += 1\n",
    "len(chinese_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65060"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chinese_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/chinese_freqs_lines.json', 'w') as outfile:\n",
    "    json.dump(chinese_tokens_unique, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seuil_zh = int(num_long_lines*0.005/100)\n",
    "seuil_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12928"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(chinese_tokens_unique))\n",
    "\n",
    "selected_chinese_tokens = []\n",
    "\n",
    "for tok in chinese_tokens_unique:\n",
    "    if chinese_tokens_unique[tok] >= seuil_zh:\n",
    "        selected_chinese_tokens.append(tok)\n",
    "len(selected_chinese_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2853863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5715225"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines = 0\n",
    "num_long_lines = 0\n",
    "path_arabic = 'data/ar.txt'\n",
    "\n",
    "with open(path_arabic) as infile:\n",
    "    for line in infile:\n",
    "        num_lines += 1\n",
    "        if len(line)>5:\n",
    "            num_long_lines += 1\n",
    "print(num_long_lines)\n",
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 65985.41222953796\n",
      "2.0 66255.21002721786\n",
      "3.0 66543.65768647194\n",
      "4.0 66747.40564274788\n",
      "5.0 66937.59504151344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58052"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_tokens = dict()\n",
    "arabic_tokens_unique = dict()\n",
    "cpt = 0\n",
    "\n",
    "with open(path_arabic) as infile:\n",
    "    for line in infile:\n",
    "        cpt += 1\n",
    "        if cpt%1000000==0:\n",
    "            print(cpt/1000000, time.time()-t0)\n",
    "        if len(line)>5:\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            for token in tokens:\n",
    "                if token not in arabic_tokens:\n",
    "                    arabic_tokens[token] = 1\n",
    "                else:\n",
    "                    arabic_tokens[token] += 1\n",
    "            for token in list(set(tokens)):\n",
    "                if token not in arabic_tokens_unique:\n",
    "                    arabic_tokens_unique[token] = 1\n",
    "                else:\n",
    "                    arabic_tokens_unique[token] += 1\n",
    "len(arabic_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/arabic_freqs.json', 'w') as outfile:\n",
    "    json.dump(arabic_tokens, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/arabic_freqs_lines.json', 'w') as outfile:\n",
    "    json.dump(arabic_tokens_unique, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seuil_ar = int(num_long_lines*0.005/100)\n",
    "seuil_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7292"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(arabic_tokens_unique))\n",
    "\n",
    "selected_arabic_tokens = []\n",
    "\n",
    "for tok in arabic_tokens_unique:\n",
    "    if arabic_tokens_unique[tok] >= seuil_ar:\n",
    "        selected_arabic_tokens.append(tok)\n",
    "len(selected_arabic_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Russe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11510609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24412492"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines = 0\n",
    "num_long_lines = 0\n",
    "path_russe = 'data/ru.txt'\n",
    "\n",
    "with open(path_russe) as infile:\n",
    "    for line in infile:\n",
    "        num_lines += 1\n",
    "        if len(line)>5:\n",
    "            num_long_lines += 1\n",
    "print(num_long_lines)\n",
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 67441.4977927208\n",
      "2.0 67781.51218128204\n",
      "3.0 68125.64979195595\n",
      "4.0 68482.1391248703\n",
      "5.0 68855.19577002525\n",
      "6.0 69165.49130535126\n",
      "7.0 69496.68894863129\n",
      "8.0 69766.70977449417\n",
      "9.0 70017.82785439491\n",
      "10.0 70265.03732800484\n",
      "11.0 70504.1641740799\n",
      "12.0 70732.79295420647\n",
      "13.0 70970.14817857742\n",
      "14.0 71204.29126548767\n",
      "15.0 71450.25069975853\n",
      "16.0 71685.4098341465\n",
      "17.0 71919.06532096863\n",
      "18.0 72156.71354842186\n",
      "19.0 72406.77476096153\n",
      "20.0 72658.3042345047\n",
      "21.0 72908.02834582329\n",
      "22.0 73165.59919810295\n",
      "23.0 73424.83898615837\n",
      "24.0 73684.35419297218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79241"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "russe_tokens = dict()\n",
    "russe_tokens_unique = dict()\n",
    "cpt = 0\n",
    "\n",
    "with open(path_russe) as infile:\n",
    "    for line in infile:\n",
    "        cpt += 1\n",
    "        if cpt%1000000==0:\n",
    "            print(cpt/1000000, time.time()-t0)\n",
    "        if len(line)>5:\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            for token in tokens:\n",
    "                if token not in russe_tokens:\n",
    "                    russe_tokens[token] = 1\n",
    "                else:\n",
    "                    russe_tokens[token] += 1\n",
    "            for token in list(set(tokens)):\n",
    "                if token not in russe_tokens_unique:\n",
    "                    russe_tokens_unique[token] = 1\n",
    "                else:\n",
    "                    russe_tokens_unique[token] += 1\n",
    "len(russe_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/russe_freqs.json', 'w') as outfile:\n",
    "    json.dump(russe_tokens, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/russe_freqs_lines.json', 'w') as outfile:\n",
    "    json.dump(russe_tokens_unique, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "575"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seuil_ru = int(num_long_lines*0.005/100)\n",
    "seuil_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14270"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(russe_tokens_unique))\n",
    "\n",
    "selected_russe_tokens = []\n",
    "\n",
    "for tok in russe_tokens_unique:\n",
    "    if russe_tokens_unique[tok] >= seuil_ru:\n",
    "        selected_russe_tokens.append(tok)\n",
    "len(selected_russe_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vietnamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3538941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7520180"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines = 0\n",
    "num_long_lines = 0\n",
    "path_vi = 'data/vi.txt'\n",
    "\n",
    "with open(path_vi) as infile:\n",
    "    for line in infile:\n",
    "        num_lines += 1\n",
    "        if len(line)>5:\n",
    "            num_long_lines += 1\n",
    "print(num_long_lines)\n",
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 98122.46129226685\n",
      "2.0 98327.61943459511\n",
      "3.0 98463.24028873444\n",
      "4.0 98585.39857244492\n",
      "5.0 98707.9568901062\n",
      "6.0 98833.27023386955\n",
      "7.0 98938.17175722122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65103"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi_tokens = dict()\n",
    "vi_tokens_unique = dict()\n",
    "cpt = 0\n",
    "\n",
    "with open(path_vi) as infile:\n",
    "    for line in infile:\n",
    "        cpt += 1\n",
    "        if cpt%1000000==0:\n",
    "            print(cpt/1000000, time.time()-t0)\n",
    "        if len(line)>5:\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            for token in tokens:\n",
    "                if token not in vi_tokens:\n",
    "                    vi_tokens[token] = 1\n",
    "                else:\n",
    "                    vi_tokens[token] += 1\n",
    "            for token in list(set(tokens)):\n",
    "                if token not in vi_tokens_unique:\n",
    "                    vi_tokens_unique[token] = 1\n",
    "                else:\n",
    "                    vi_tokens_unique[token] += 1\n",
    "len(vi_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/vi_freqs.json', 'w') as outfile:\n",
    "    json.dump(vi_tokens, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/vi_freqs_lines.json', 'w') as outfile:\n",
    "    json.dump(vi_tokens_unique, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seuil_vi = int(num_long_lines*0.005/100)\n",
    "seuil_vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17512"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(vi_tokens_unique))\n",
    "\n",
    "selected_vi_tokens = []\n",
    "\n",
    "for tok in vi_tokens_unique:\n",
    "    if vi_tokens_unique[tok] >= seuil_vi:\n",
    "        selected_vi_tokens.append(tok)\n",
    "len(selected_vi_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1097507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2278048"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines = 0\n",
    "num_long_lines = 0\n",
    "path_el = 'data/el.txt'\n",
    "\n",
    "with open(path_el) as infile:\n",
    "    for line in infile:\n",
    "        num_lines += 1\n",
    "        if len(line)>5:\n",
    "            num_long_lines += 1\n",
    "print(num_long_lines)\n",
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 99542.60071659088\n",
      "2.0 100057.30239129066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54387"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el_tokens = dict()\n",
    "el_tokens_unique = dict()\n",
    "cpt = 0\n",
    "\n",
    "with open(path_el) as infile:\n",
    "    for line in infile:\n",
    "        cpt += 1\n",
    "        if cpt%1000000==0:\n",
    "            print(cpt/1000000, time.time()-t0)\n",
    "        if len(line)>5:\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            for token in tokens:\n",
    "                if token not in el_tokens:\n",
    "                    el_tokens[token] = 1\n",
    "                else:\n",
    "                    el_tokens[token] += 1\n",
    "            for token in list(set(tokens)):\n",
    "                if token not in el_tokens_unique:\n",
    "                    el_tokens_unique[token] = 1\n",
    "                else:\n",
    "                    el_tokens_unique[token] += 1\n",
    "len(el_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/el_freqs.json', 'w') as outfile:\n",
    "    json.dump(el_tokens, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/el_freqs_lines.json', 'w') as outfile:\n",
    "    json.dump(el_tokens_unique, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seuil_el = int(num_long_lines*0.005/100)\n",
    "seuil_el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11616"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(el_tokens_unique))\n",
    "\n",
    "selected_el_tokens = []\n",
    "\n",
    "for tok in el_tokens_unique:\n",
    "    if el_tokens_unique[tok] >= seuil_el:\n",
    "        selected_el_tokens.append(tok)\n",
    "len(selected_el_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulgarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1394130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2976901"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines = 0\n",
    "num_long_lines = 0\n",
    "path_bg = 'data/bg.txt'\n",
    "\n",
    "with open(path_bg) as infile:\n",
    "    for line in infile:\n",
    "        num_lines += 1\n",
    "        if len(line)>5:\n",
    "            num_long_lines += 1\n",
    "print(num_long_lines)\n",
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 140929.85722017288\n",
      "2.0 141206.18313527107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54431"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg_tokens = dict()\n",
    "bg_tokens_unique = dict()\n",
    "cpt = 0\n",
    "\n",
    "with open(path_bg) as infile:\n",
    "    for line in infile:\n",
    "        cpt += 1\n",
    "        if cpt%1000000==0:\n",
    "            print(cpt/1000000, time.time()-t0)\n",
    "        if len(line)>5:\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            for token in tokens:\n",
    "                if token not in bg_tokens:\n",
    "                    bg_tokens[token] = 1\n",
    "                else:\n",
    "                    bg_tokens[token] += 1\n",
    "            for token in list(set(tokens)):\n",
    "                if token not in bg_tokens_unique:\n",
    "                    bg_tokens_unique[token] = 1\n",
    "                else:\n",
    "                    bg_tokens_unique[token] += 1\n",
    "len(bg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/bg_freqs.json', 'w') as outfile:\n",
    "    json.dump(bg_tokens, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/bg_freqs_lines.json', 'w') as outfile:\n",
    "    json.dump(bg_tokens_unique, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seuil_bg = int(num_long_lines*0.005/100)\n",
    "seuil_bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12121"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(bg_tokens_unique))\n",
    "\n",
    "selected_bg_tokens = []\n",
    "\n",
    "for tok in bg_tokens_unique:\n",
    "    if bg_tokens_unique[tok] >= seuil_bg:\n",
    "        selected_bg_tokens.append(tok)\n",
    "len(selected_bg_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1594166"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines = 0\n",
    "num_long_lines = 0\n",
    "path_th = 'data/th.txt'\n",
    "\n",
    "with open(path_th) as infile:\n",
    "    for line in infile:\n",
    "        num_lines += 1\n",
    "        if len(line)>5:\n",
    "            num_long_lines += 1\n",
    "print(num_long_lines)\n",
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 105579.57854032516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45346"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_tokens = dict()\n",
    "th_tokens_unique = dict()\n",
    "cpt = 0\n",
    "\n",
    "with open(path_th) as infile:\n",
    "    for line in infile:\n",
    "        cpt += 1\n",
    "        if cpt%1000000==0:\n",
    "            print(cpt/1000000, time.time()-t0)\n",
    "        if len(line)>5:\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            for token in tokens:\n",
    "                if token not in th_tokens:\n",
    "                    th_tokens[token] = 1\n",
    "                else:\n",
    "                    th_tokens[token] += 1\n",
    "            for token in list(set(tokens)):\n",
    "                if token not in th_tokens_unique:\n",
    "                    th_tokens_unique[token] = 1\n",
    "                else:\n",
    "                    th_tokens_unique[token] += 1\n",
    "len(th_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/th_freqs.json', 'w') as outfile:\n",
    "    json.dump(th_tokens, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/th_freqs_lines.json', 'w') as outfile:\n",
    "    json.dump(th_tokens_unique, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "seuil_th = int(num_long_lines*0.005/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8493"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(th_tokens_unique))\n",
    "\n",
    "selected_th_tokens = []\n",
    "\n",
    "for tok in th_tokens_unique:\n",
    "    if th_tokens_unique[tok] >= seuil_th:\n",
    "        selected_th_tokens.append(tok)\n",
    "len(selected_th_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turkish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1708074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3576847"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines = 0\n",
    "num_long_lines = 0\n",
    "path_tr = 'data/tr.txt'\n",
    "\n",
    "with open(path_tr) as infile:\n",
    "    for line in infile:\n",
    "        num_lines += 1\n",
    "        if len(line)>5:\n",
    "            num_long_lines += 1\n",
    "print(num_long_lines)\n",
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 106623.1616370678\n",
      "2.0 106868.95719408989\n",
      "3.0 107085.14948058128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45346"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_tokens = dict()\n",
    "tr_tokens_unique = dict()\n",
    "cpt = 0\n",
    "\n",
    "with open(path_tr) as infile:\n",
    "    for line in infile:\n",
    "        cpt += 1\n",
    "        if cpt%1000000==0:\n",
    "            print(cpt/1000000, time.time()-t0)\n",
    "        if len(line)>5:\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            for token in tokens:\n",
    "                if token not in tr_tokens:\n",
    "                    tr_tokens[token] = 1\n",
    "                else:\n",
    "                    tr_tokens[token] += 1\n",
    "            for token in list(set(tokens)):\n",
    "                if token not in tr_tokens_unique:\n",
    "                    tr_tokens_unique[token] = 1\n",
    "                else:\n",
    "                    tr_tokens_unique[token] += 1\n",
    "len(th_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/tr_freqs.json', 'w') as outfile:\n",
    "    json.dump(tr_tokens, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/tr_freqs_lines.json', 'w') as outfile:\n",
    "    json.dump(tr_tokens_unique, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seuil_tr = int(num_long_lines*0.005/100)\n",
    "seuil_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19086"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tr_tokens_unique))\n",
    "\n",
    "selected_tr_tokens = []\n",
    "\n",
    "for tok in tr_tokens_unique:\n",
    "    if tr_tokens_unique[tok] >= seuil_tr:\n",
    "        selected_tr_tokens.append(tok)\n",
    "len(selected_tr_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1192644"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines = 0\n",
    "num_long_lines = 0\n",
    "path_hi = 'data/hi.txt'\n",
    "\n",
    "with open(path_hi) as infile:\n",
    "    for line in infile:\n",
    "        num_lines += 1\n",
    "        if len(line)>5:\n",
    "            num_long_lines += 1\n",
    "print(num_long_lines)\n",
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 107493.82563185692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45346"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi_tokens = dict()\n",
    "hi_tokens_unique = dict()\n",
    "cpt = 0\n",
    "\n",
    "with open(path_hi) as infile:\n",
    "    for line in infile:\n",
    "        cpt += 1\n",
    "        if cpt%1000000==0:\n",
    "            print(cpt/1000000, time.time()-t0)\n",
    "        if len(line)>5:\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            for token in tokens:\n",
    "                if token not in hi_tokens:\n",
    "                    hi_tokens[token] = 1\n",
    "                else:\n",
    "                    hi_tokens[token] += 1\n",
    "            for token in list(set(tokens)):\n",
    "                if token not in hi_tokens_unique:\n",
    "                    hi_tokens_unique[token] = 1\n",
    "                else:\n",
    "                    hi_tokens_unique[token] += 1\n",
    "len(th_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/hi_freqs.json', 'w') as outfile:\n",
    "    json.dump(hi_tokens, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/hi_freqs_lines.json', 'w') as outfile:\n",
    "    json.dump(hi_tokens_unique, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seuil_hi = int(num_long_lines*0.005/100)\n",
    "seuil_hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5664"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(hi_tokens_unique))\n",
    "\n",
    "selected_hi_tokens = []\n",
    "\n",
    "for tok in hi_tokens_unique:\n",
    "    if hi_tokens_unique[tok] >= seuil_hi:\n",
    "        selected_hi_tokens.append(tok)\n",
    "len(selected_hi_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Urdu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "897328"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines = 0\n",
    "num_long_lines = 0\n",
    "path_ur = 'data/ur.txt'\n",
    "\n",
    "with open(path_ur) as infile:\n",
    "    for line in infile:\n",
    "        num_lines += 1\n",
    "        if len(line)>5:\n",
    "            num_long_lines += 1\n",
    "print(num_long_lines)\n",
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35702"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ur_tokens = dict()\n",
    "ur_tokens_unique = dict()\n",
    "cpt = 0\n",
    "\n",
    "with open(path_ur) as infile:\n",
    "    for line in infile:\n",
    "        cpt += 1\n",
    "        if cpt%1000000==0:\n",
    "            print(cpt/1000000, time.time()-t0)\n",
    "        if len(line)>5:\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            for token in tokens:\n",
    "                if token not in ur_tokens:\n",
    "                    ur_tokens[token] = 1\n",
    "                else:\n",
    "                    ur_tokens[token] += 1\n",
    "            for token in list(set(tokens)):\n",
    "                if token not in ur_tokens_unique:\n",
    "                    ur_tokens_unique[token] = 1\n",
    "                else:\n",
    "                    ur_tokens_unique[token] += 1\n",
    "len(ur_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/ur_freqs.json', 'w') as outfile:\n",
    "    json.dump(ur_tokens, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/ur_freqs_lines.json', 'w') as outfile:\n",
    "    json.dump(ur_tokens_unique, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seuil_ur = int(num_long_lines*0.005/100)\n",
    "seuil_ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8656"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(ur_tokens_unique))\n",
    "\n",
    "selected_ur_tokens = []\n",
    "\n",
    "for tok in ur_tokens_unique:\n",
    "    if ur_tokens_unique[tok] >= seuil_ur:\n",
    "        selected_ur_tokens.append(tok)\n",
    "len(selected_ur_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swahili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "350193"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines = 0\n",
    "num_long_lines = 0\n",
    "path_sw = 'data/sw.txt'\n",
    "\n",
    "with open(path_sw) as infile:\n",
    "    for line in infile:\n",
    "        num_lines += 1\n",
    "        if len(line)>5:\n",
    "            num_long_lines += 1\n",
    "print(num_long_lines)\n",
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34536"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw_tokens = dict()\n",
    "sw_tokens_unique = dict()\n",
    "cpt = 0\n",
    "\n",
    "with open(path_sw) as infile:\n",
    "    for line in infile:\n",
    "        cpt += 1\n",
    "        if cpt%1000000==0:\n",
    "            print(cpt/1000000, time.time()-t0)\n",
    "        if len(line)>5:\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            for token in tokens:\n",
    "                if token not in sw_tokens:\n",
    "                    sw_tokens[token] = 1\n",
    "                else:\n",
    "                    sw_tokens[token] += 1\n",
    "            for token in list(set(tokens)):\n",
    "                if token not in sw_tokens_unique:\n",
    "                    sw_tokens_unique[token] = 1\n",
    "                else:\n",
    "                    sw_tokens_unique[token] += 1\n",
    "len(sw_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/sw_freqs.json', 'w') as outfile:\n",
    "    json.dump(sw_tokens, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens_freqs/sw_freqs_lines.json', 'w') as outfile:\n",
    "    json.dump(sw_tokens_unique, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seuil_sw = int(num_long_lines*0.005/100)\n",
    "seuil_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16619"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sw_tokens_unique))\n",
    "\n",
    "selected_sw_tokens = []\n",
    "\n",
    "for tok in sw_tokens_unique:\n",
    "    if sw_tokens_unique[tok] >= seuil_sw:\n",
    "        selected_sw_tokens.append(tok)\n",
    "len(selected_sw_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langs = {'en': selected_english_tokens, 'fr': selected_french_tokens, 'es': selected_spanish_tokens,\n",
    "         'de': selected_german_tokens, 'zh': selected_chinese_tokens, 'ar': selected_arabic_tokens,\n",
    "         'ru': selected_russe_tokens, 'vi': selected_vi_tokens, 'el': selected_el_tokens,\n",
    "         'bg': selected_bg_tokens, 'th': selected_th_tokens, 'tr': selected_tr_tokens,\n",
    "         'hi': selected_hi_tokens, 'ur': selected_ur_tokens, 'sw': selected_sw_tokens}\n",
    "len(langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in langs.keys():\n",
    "    with open('selected_tokens/selected_'+lang+'_tokens.txt', 'w') as output:\n",
    "        for tok in langs[lang]:\n",
    "            output.write(tok+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langs = dict()\n",
    "\n",
    "for l in ['en', 'fr', 'es', 'de', 'zh', 'ar', 'ru', 'vi', 'el', 'bg', 'th', 'tr', 'hi', 'ur', 'sw']:\n",
    "    with open('selected_tokens/selected_'+l+'_tokens.txt') as file:\n",
    "        langs[l] = file.read().splitlines()\n",
    "len(langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71564"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_selected_tokens = []\n",
    "for k in langs.keys():\n",
    "    all_selected_tokens.extend(langs[k])\n",
    "selected_tokens = list(set(all_selected_tokens))\n",
    "len(selected_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71577"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENS_TO_KEEP = ['[PAD]','[UNK]','[CLS]','[SEP]','[MASK]','[unused1]','[unused2]','[unused3]',\n",
    "                  '[unused4]','[unused5]', '[unused6]','[unused7]','[unused8]','[unused9]']\n",
    "\n",
    "for tok in TOKENS_TO_KEEP:\n",
    "    if tok not in selected_tokens:\n",
    "        selected_tokens.append(tok)\n",
    "\n",
    "len(selected_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_embeddings(model, old_vocab, new_vocab, model_name='new_model'):\n",
    "    \n",
    "    # Get old embeddings from model\n",
    "    old_embeddings = model.get_input_embeddings()\n",
    "    old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n",
    "    \n",
    "    if old_num_tokens != len(old_vocab):\n",
    "        print('len(old_vocab) != len(model.old_embeddings)')\n",
    "        return old_embeddings\n",
    "    \n",
    "    new_num_tokens = len(new_vocab)\n",
    "    if new_vocab is None:\n",
    "        print('nothing to copy')\n",
    "        return old_embeddings\n",
    "    \n",
    "    # Build new embeddings\n",
    "    new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim)\n",
    "    new_embeddings.to(old_embeddings.weight.device)\n",
    "    \n",
    "    # Copy weights\n",
    "    i = 0\n",
    "    j = 0\n",
    "    vocab = []\n",
    "    for token in old_vocab:\n",
    "        if token in new_vocab:\n",
    "            vocab.append(token)\n",
    "            new_embeddings.weight.data[i, :] = old_embeddings.weight.data[j, :]\n",
    "            i += 1\n",
    "        j += 1\n",
    "    \n",
    "    model.set_input_embeddings(new_embeddings)\n",
    "    \n",
    "    # Update base model and current model config\n",
    "    model.config.vocab_size = new_num_tokens\n",
    "    model.vocab_size = new_num_tokens\n",
    "\n",
    "    # Tie weights\n",
    "    model.tie_weights()\n",
    "    \n",
    "    # Save new model\n",
    "    model.save_pretrained(model_name)\n",
    "    print(model_name, \" - \", \" num_parameters : \", model.num_parameters())\n",
    "    print(model_name, \" - \", \" num_tokens : \", len(vocab))\n",
    "    \n",
    "    # Save vocab\n",
    "    fw = open(os.path.join(model_name, 'vocab.txt'), 'w')\n",
    "    for token in vocab:\n",
    "        fw.write(token+'\\n')\n",
    "    fw.close()\n",
    "    \n",
    "    # Save tokenizer config\n",
    "    fw = open(os.path.join(model_name, 'tokenizer_config.json'), 'w')\n",
    "    json.dump({\"do_lower_case\": False, \"model_max_length\": 512}, fw)\n",
    "    fw.close()\n",
    "    \n",
    "    return new_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "177974523"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "model_cased.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating 15langs model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-15lang-cased  -   num_parameters :  141085593\n",
      "new-models/bert-base-15lang-cased  -   num_tokens :  71577\n",
      "307.41669940948486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(71577, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, selected_tokens, 'new-models/bert-base-15lang-cased')\n",
    "print(time.time()-t)\n",
    "new_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating monolingual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-cased  -   num_parameters :  107937079\n",
      "new-models/bert-base-en-cased  -   num_tokens :  28471\n",
      "73.92174339294434\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-fr-cased  -   num_parameters :  104879535\n",
      "new-models/bert-base-fr-cased  -   num_tokens :  24495\n",
      "65.44030809402466\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-es-cased  -   num_parameters :  106312951\n",
      "new-models/bert-base-es-cased  -   num_tokens :  26359\n",
      "69.25197982788086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-de-cased  -   num_parameters :  106070716\n",
      "new-models/bert-base-de-cased  -   num_tokens :  26044\n",
      "70.63011884689331\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-zh-cased  -   num_parameters :  95994509\n",
      "new-models/bert-base-zh-cased  -   num_tokens :  12941\n",
      "32.777645111083984\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-ar-cased  -   num_parameters :  91660425\n",
      "new-models/bert-base-ar-cased  -   num_tokens :  7305\n",
      "19.091326236724854\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-ru-cased  -   num_parameters :  97026507\n",
      "new-models/bert-base-ru-cased  -   num_tokens :  14283\n",
      "41.2095103263855\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-vi-cased  -   num_parameters :  99519605\n",
      "new-models/bert-base-vi-cased  -   num_tokens :  17525\n",
      "46.40220260620117\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-el-cased  -   num_parameters :  94985581\n",
      "new-models/bert-base-el-cased  -   num_tokens :  11629\n",
      "27.513412714004517\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-bg-cased  -   num_parameters :  95373926\n",
      "new-models/bert-base-bg-cased  -   num_tokens :  12134\n",
      "36.462172985076904\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-th-cased  -   num_parameters :  92583994\n",
      "new-models/bert-base-th-cased  -   num_tokens :  8506\n",
      "23.689405918121338\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-tr-cased  -   num_parameters :  100730011\n",
      "new-models/bert-base-tr-cased  -   num_tokens :  19099\n",
      "48.08704328536987\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-hi-cased  -   num_parameters :  90408493\n",
      "new-models/bert-base-hi-cased  -   num_tokens :  5677\n",
      "17.46791982650757\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-ur-cased  -   num_parameters :  92709341\n",
      "new-models/bert-base-ur-cased  -   num_tokens :  8669\n",
      "23.566728591918945\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-sw-cased  -   num_parameters :  98832888\n",
      "new-models/bert-base-sw-cased  -   num_tokens :  16632\n",
      "44.70777344703674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lang in list(langs.keys()):\n",
    "    del model_cased\n",
    "    model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "    t = time.time()\n",
    "    new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs[lang]+TOKENS_TO_KEEP)), \n",
    "                                 'new-models/bert-base-'+lang+'-cased')\n",
    "    print(time.time()-t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating bilingual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en-fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-fr-cased  -   num_parameters :  111732863\n",
      "new-models/bert-base-en-fr-cased  -   num_tokens :  33407\n",
      "65.13403367996216\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-fr-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_cust = BertTokenizer.from_pretrained('new-models/bert-base-en-fr-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNK]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer_cust.unk_token)\n",
    "tokenizer_cust.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEP]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer_cust.sep_token)\n",
    "tokenizer_cust.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer_cust.mask_token)\n",
    "tokenizer_cust.mask_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en-es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-en-es-cased  -   num_parameters :  113245486\n",
      "bert-base-en-es-cased  -   num_tokens :  35374\n",
      "70.03286385536194\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['es']+TOKENS_TO_KEEP)), \n",
    "                             'bert-base-en-es-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en-de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-en-de-cased  -   num_parameters :  113145516\n",
      "bert-base-en-de-cased  -   num_tokens :  35244\n",
      "64.38058757781982\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['de']+TOKENS_TO_KEEP)), \n",
    "                             'bert-base-en-de-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en-el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-en-el-cased  -   num_parameters :  109559669\n",
      "bert-base-en-el-cased  -   num_tokens :  30581\n",
      "61.141303062438965\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['el']+TOKENS_TO_KEEP)), \n",
    "                             'bert-base-en-el-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en-bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-en-bg-cased  -   num_parameters :  112664891\n",
      "bert-base-en-bg-cased  -   num_tokens :  34619\n",
      "55.813469648361206\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['bg']+TOKENS_TO_KEEP)), \n",
    "                             'bert-base-en-bg-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en-ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-en-ru-cased  -   num_parameters :  114853465\n",
      "bert-base-en-ru-cased  -   num_tokens :  37465\n",
      "51.599135398864746\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['ru']+TOKENS_TO_KEEP)), \n",
    "                             'bert-base-en-ru-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en-tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-en-tr-cased  -   num_parameters :  110167948\n",
      "bert-base-en-tr-cased  -   num_tokens :  31372\n",
      "43.49697542190552\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['tr']+TOKENS_TO_KEEP)), \n",
    "                             'bert-base-en-tr-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en-ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-en-ar-cased  -   num_parameters :  110630117\n",
      "bert-base-en-ar-cased  -   num_tokens :  31973\n",
      "47.8929979801178\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['ar']+TOKENS_TO_KEEP)), \n",
    "                             'bert-base-en-ar-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en-vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-en-vi-cased  -   num_parameters :  110210243\n",
      "bert-base-en-vi-cased  -   num_tokens :  31427\n",
      "43.392895460128784\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['vi']+TOKENS_TO_KEEP)), \n",
    "                             'bert-base-en-vi-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en-th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-en-th-cased  -   num_parameters :  108459999\n",
      "bert-base-en-th-cased  -   num_tokens :  29151\n",
      "40.50568199157715\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['th']+TOKENS_TO_KEEP)), \n",
    "                             'bert-base-en-th-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en-zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-en-zh-cased  -   num_parameters :  113130905\n",
      "bert-base-en-zh-cased  -   num_tokens :  35225\n",
      "55.543163537979126\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['zh']+TOKENS_TO_KEEP)), \n",
    "                             'bert-base-en-zh-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en-hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-en-hi-cased  -   num_parameters :  109165172\n",
      "bert-base-en-hi-cased  -   num_tokens :  30068\n",
      "59.588555097579956\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['hi']+TOKENS_TO_KEEP)), \n",
    "                             'bert-base-en-hi-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en-sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-en-sw-cased  -   num_parameters :  109641952\n",
      "bert-base-en-sw-cased  -   num_tokens :  30688\n",
      "50.077721118927\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['sw']+TOKENS_TO_KEEP)), \n",
    "                             'bert-base-en-sw-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en-ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-en-ur-cased  -   num_parameters :  110167179\n",
      "bert-base-en-ur-cased  -   num_tokens :  31371\n",
      "58.7756142616272\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['ur']+TOKENS_TO_KEEP)), \n",
    "                             'bert-base-en-ur-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare original and new models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "177974523"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107937079"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new model\n",
    "tokenizer_cust = BertTokenizer.from_pretrained('new-models/bert-base-en-cased')\n",
    "model_cust = BertForMaskedLM.from_pretrained('new-models/bert-base-en-cased')\n",
    "model_cust.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28471"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_cust.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(28471, 768, padding_idx=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cust.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love NLP\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output_original = model(**encoded_input)\n",
    "encoded_input_cust = tokenizer_cust(text, return_tensors='pt')\n",
    "output_cust = model_cust(**encoded_input_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   146, 16138, 81130, 11127,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   11,    54,  3477, 23039,   892,    12]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input_cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6\n"
     ]
    }
   ],
   "source": [
    "print(len(output_original[0][0]), len(output_cust[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.5026, -8.4598, -8.5441,  ..., -8.4676, -8.3309, -8.4011],\n",
       "        [-7.3152, -7.4170, -7.2602,  ..., -6.7312, -7.1424, -7.0654],\n",
       "        [-8.5618, -9.1271, -7.8516,  ..., -8.3914, -7.0207, -8.6194],\n",
       "        [-6.9560, -6.5613, -6.1853,  ..., -6.7822, -6.2483, -6.3508],\n",
       "        [-8.5762, -7.8520, -6.7847,  ..., -8.3420, -6.3392, -7.7183],\n",
       "        [-7.3157, -7.2129, -6.7588,  ..., -6.8620, -6.5756, -8.0586]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_original[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.5026, -8.4598, -8.5441,  ..., -2.4711, -4.2386, -1.1125],\n",
       "        [-7.3152, -7.4170, -7.2602,  ..., -2.6856, -5.7124, -3.6802],\n",
       "        [-8.5618, -9.1271, -7.8516,  ...,  0.5088, -6.1223,  0.1668],\n",
       "        [-6.9560, -6.5613, -6.1853,  ..., -2.0895, -4.5995, -0.7040],\n",
       "        [-8.5762, -7.8520, -6.7847,  ..., -1.4740, -3.6588, -0.5311],\n",
       "        [-7.3157, -7.2129, -6.7588,  ..., -2.2422, -5.2254,  1.2093]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_cust[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "[-8.502596 -8.459798 -8.544106 -8.420239 -8.55526 ]\n",
      "[-8.502596 -8.459798 -8.544106 -8.420239 -8.55526 ]\n",
      "\n",
      "I\n",
      "[-7.3151646 -7.416972  -7.260161  -7.000843  -7.0258822]\n",
      "[-7.3151646 -7.416972  -7.260161  -7.000843  -7.0258822]\n",
      "\n",
      "love\n",
      "[-8.561801  -9.127073  -7.8515744 -8.405504  -8.349455 ]\n",
      "[-8.561801  -9.127073  -7.8515744 -8.405504  -8.349455 ]\n",
      "\n",
      "NL\n",
      "[-6.9560323 -6.5612655 -6.185295  -5.8626823 -6.8318934]\n",
      "[-6.9560323 -6.5612655 -6.185295  -5.8626823 -6.8318934]\n",
      "\n",
      "##P\n",
      "[-8.576168 -7.852015 -6.784669 -7.650504 -7.808926]\n",
      "[-8.576168 -7.852015 -6.784669 -7.650504 -7.808926]\n",
      "\n",
      "[SEP]\n",
      "[-7.315665  -7.2128882 -6.75876   -6.995212  -7.240693 ]\n",
      "[-7.315665  -7.2128882 -6.75876   -6.995212  -7.240693 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for input_id in encoded_input['input_ids'][0]:\n",
    "    print(tokenizer.convert_ids_to_tokens(int(input_id)))\n",
    "    print(output_original[0][0][i].detach().numpy()[:5])\n",
    "    print(output_cust[0][0][i].detach().numpy()[:5])\n",
    "    print()\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests on MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital 0.6365790963172913\n",
      "city 0.08376165479421616\n",
      "City 0.034411922097206116\n",
      "port 0.02745007537305355\n",
      "centre 0.012592659331858158\n"
     ]
    }
   ],
   "source": [
    "## declare task ##\n",
    "pipe = pipeline(task=\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "## example ##\n",
    "input_  = 'Paris is the [MASK] of France.'\n",
    "\n",
    "output_ = pipe(input_)\n",
    "for i in range(len(output_)):\n",
    "    print(output_[i]['token_str'], output_[i]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital 0.6138291358947754\n",
      "city 0.07793192565441132\n",
      "City 0.03626968339085579\n",
      "port 0.02652185596525669\n",
      "centre 0.014177550561726093\n"
     ]
    }
   ],
   "source": [
    "## declare task ##\n",
    "pipe = pipeline(task=\"fill-mask\", model=model_cust, tokenizer=tokenizer_cust)\n",
    "\n",
    "## example ##\n",
    "input_  = 'Paris is the [MASK] of France.'\n",
    "\n",
    "output_ = pipe(input_)\n",
    "for i in range(len(output_)):\n",
    "    print(output_[i]['token_str'], output_[i]['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert all models to TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in os.listdir('new-models'):\n",
    "    tf_model = TFBertForMaskedLM.from_pretrained(\"new-models/\"+model_name, from_pt=True)\n",
    "    tf_model.save_pretrained(\"new-models/\"+model_name)\n",
    "    del tf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
