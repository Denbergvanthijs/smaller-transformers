{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python@3.9/lib/python3.9/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from torch import nn\n",
    "from transformers import pipeline\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download and extract the [2018 Wikipedia dumps](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2735) in the data folder for the 15 XNLI languages :\n",
    "\n",
    "```bash\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/en.txt.gz -P data\n",
    "gunzip data/en.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/fr.txt.gz -P data\n",
    "gunzip data/fr.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/es.txt.gz -P data\n",
    "gunzip data/es.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/de.txt.gz -P data\n",
    "gunzip data/de.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/zh.txt.gz -P data\n",
    "gunzip data/zh.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/ar.txt.gz -P data\n",
    "gunzip data/ar.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/ru.txt.gz -P data\n",
    "gunzip data/ru.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/vi.txt.gz -P data\n",
    "gunzip data/vi.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/el.txt.gz -P data\n",
    "gunzip data/el.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/bg.txt.gz -P data\n",
    "gunzip data/bg.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/th.txt.gz -P data\n",
    "gunzip data/th.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/tr.txt.gz -P data\n",
    "gunzip data/tr.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/hi.txt.gz -P data\n",
    "gunzip data/hi.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/ur.txt.gz -P data\n",
    "gunzip data/ur.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/sw.txt.gz -P data\n",
    "gunzip data/sw.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/nl.txt.gz -P data\n",
    "gunzip data/nl.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/uk.txt.gz -P data\n",
    "gunzip data/uk.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/ro.txt.gz -P data\n",
    "gunzip data/ro.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/pt.txt.gz -P data\n",
    "gunzip data/pt.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/it.txt.gz -P data\n",
    "gunzip data/it.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/lt.txt.gz -P data\n",
    "gunzip data/lt.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/no.txt.gz -P data\n",
    "gunzip data/no.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/pl.txt.gz -P data\n",
    "gunzip data/pl.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/da.txt.gz -P data\n",
    "gunzip data/da.txt.gz\n",
    "wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2735/ja.txt.gz -P data\n",
    "gunzip data/ja.txt.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_vocab = list(tokenizer.vocab.keys())\n",
    "len(bert_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = ['en', 'fr', 'es', 'de', 'zh', 'ar', 'ru', 'vi', 'el', 'bg', 'th', 'tr', 'hi', \n",
    "              'ur', 'sw', 'nl', 'uk', 'ro', 'pt', 'it', 'lt', 'no', 'pl', 'da', 'ja']\n",
    "len(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    num_lines = 0\n",
    "    num_long_lines = 0\n",
    "    path = 'data/'+lang+'.txt'\n",
    "    with open(path) as infile:\n",
    "        for line in infile:\n",
    "            num_lines += 1\n",
    "            if len(line)>5:\n",
    "                num_long_lines += 1\n",
    "    # compute frequencies\n",
    "    lang_tokens = dict()\n",
    "    lang_tokens_unique = dict()\n",
    "    t0 = time.time()\n",
    "    with open(path) as infile:\n",
    "        for line in infile:\n",
    "            if len(line)>5:\n",
    "                tokens = tokenizer.tokenize(line)\n",
    "                for token in tokens:\n",
    "                    if token not in lang_tokens:\n",
    "                        lang_tokens[token] = 1\n",
    "                    else:\n",
    "                        lang_tokens[token] += 1\n",
    "                for token in list(set(tokens)):\n",
    "                    if token not in lang_tokens_unique:\n",
    "                        lang_tokens_unique[token] = 1\n",
    "                    else:\n",
    "                        lang_tokens_unique[token] += 1\n",
    "    # save frequencies\n",
    "    with open('tokens_freqs/'+lang+'_freqs.json', 'w') as outfile:\n",
    "        json.dump(lang_tokens, outfile)\n",
    "    seuil = int(num_long_lines*0.005/100)\n",
    "    num_selected_tokens = 0\n",
    "    with open('selected_tokens/selected_'+lang+'_tokens.txt', 'w') as output:\n",
    "        for tok in lang_tokens_unique:\n",
    "            if lang_tokens_unique[tok] >= seuil:\n",
    "                output.write(tok+'\\n')\n",
    "                num_selected_tokens += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langs = dict()\n",
    "\n",
    "for l in languages:\n",
    "    with open('selected_tokens/selected_'+l+'_tokens.txt') as file:\n",
    "        langs[l] = file.read().splitlines()\n",
    "len(langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84972"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_selected_tokens = []\n",
    "for k in langs.keys():\n",
    "    all_selected_tokens.extend(langs[k])\n",
    "selected_tokens = list(set(all_selected_tokens))\n",
    "len(selected_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84985"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENS_TO_KEEP = ['[PAD]','[UNK]','[CLS]','[SEP]','[MASK]','[unused1]','[unused2]','[unused3]',\n",
    "                  '[unused4]','[unused5]', '[unused6]','[unused7]','[unused8]','[unused9]']\n",
    "\n",
    "for tok in TOKENS_TO_KEEP:\n",
    "    if tok not in selected_tokens:\n",
    "        selected_tokens.append(tok)\n",
    "\n",
    "len(selected_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_embeddings(model, old_vocab, new_vocab, model_name='new_model'):\n",
    "    \n",
    "    # Get old embeddings from model\n",
    "    old_embeddings = model.get_input_embeddings()\n",
    "    old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n",
    "    \n",
    "    if old_num_tokens != len(old_vocab):\n",
    "        print('len(old_vocab) != len(model.old_embeddings)')\n",
    "        return old_embeddings\n",
    "    \n",
    "    new_num_tokens = len(new_vocab)\n",
    "    if new_vocab is None:\n",
    "        print('nothing to copy')\n",
    "        return old_embeddings\n",
    "    \n",
    "    # Build new embeddings\n",
    "    new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim)\n",
    "    new_embeddings.to(old_embeddings.weight.device)\n",
    "    \n",
    "    # Copy weights\n",
    "    i = 0\n",
    "    j = 0\n",
    "    vocab = []\n",
    "    for token in old_vocab:\n",
    "        if token in new_vocab:\n",
    "            vocab.append(token)\n",
    "            new_embeddings.weight.data[i, :] = old_embeddings.weight.data[j, :]\n",
    "            i += 1\n",
    "        j += 1\n",
    "    \n",
    "    model.set_input_embeddings(new_embeddings)\n",
    "    \n",
    "    # Update base model and current model config\n",
    "    model.config.vocab_size = new_num_tokens\n",
    "    model.vocab_size = new_num_tokens\n",
    "\n",
    "    # Tie weights\n",
    "    model.tie_weights()\n",
    "    \n",
    "    # Save new model\n",
    "    model.save_pretrained(model_name)\n",
    "    print(model_name, \" - \", \" num_parameters : \", model.num_parameters())\n",
    "    print(model_name, \" - \", \" num_tokens : \", len(vocab))\n",
    "    \n",
    "    # Save vocab\n",
    "    fw = open(os.path.join(model_name, 'vocab.txt'), 'w')\n",
    "    for token in vocab:\n",
    "        fw.write(token+'\\n')\n",
    "    fw.close()\n",
    "    \n",
    "    # Save tokenizer config\n",
    "    fw = open(os.path.join(model_name, 'tokenizer_config.json'), 'w')\n",
    "    json.dump({\"do_lower_case\": False, \"model_max_length\": 512}, fw)\n",
    "    fw.close()\n",
    "    \n",
    "    return new_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "177974523"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "model_cased.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating 25langs model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-25lang-cased  -   num_parameters :  151396345\n",
      "new-models/bert-base-25lang-cased  -   num_tokens :  84985\n",
      "449.4667663574219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(84985, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, selected_tokens, 'new-models/bert-base-25lang-cased')\n",
    "print(time.time()-t)\n",
    "new_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating 5langs models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-fr-es-de-zh-cased  -   num_parameters :  125126536\n",
      "new-models/bert-base-en-fr-es-de-zh-cased  -   num_tokens :  50824\n",
      "158.7407102584839\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['es']+\n",
    "                                                               langs['de']+langs['zh']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-fr-es-de-zh-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-fr-nl-ru-ar-cased  -   num_parameters :  123720035\n",
      "new-models/bert-base-en-fr-nl-ru-ar-cased  -   num_tokens :  48995\n",
      "143.9530508518219\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['nl']+\n",
    "                                                               langs['ru']+langs['ar']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-fr-nl-ru-ar-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-fr-uk-el-ro-cased  -   num_parameters :  120014224\n",
      "new-models/bert-base-en-fr-uk-el-ro-cased  -   num_tokens :  44176\n",
      "120.72881722450256\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['uk']+\n",
    "                                                               langs['el']+langs['ro']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-fr-uk-el-ro-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-fr-es-pt-it-cased  -   num_parameters :  119231382\n",
      "new-models/bert-base-en-fr-es-pt-it-cased  -   num_tokens :  43158\n",
      "157.18285059928894\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['es']+\n",
    "                                                               langs['pt']+langs['it']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-fr-es-pt-it-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-fr-lt-no-pl-cased  -   num_parameters :  118497756\n",
      "new-models/bert-base-en-fr-lt-no-pl-cased  -   num_tokens :  42204\n",
      "104.5706377029419\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['lt']+\n",
    "                                                               langs['no']+langs['pl']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-fr-lt-no-pl-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-fr-zh-ja-vi-cased  -   num_parameters :  119745074\n",
      "new-models/bert-base-en-fr-zh-ja-vi-cased  -   num_tokens :  43826\n",
      "131.54477763175964\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['zh']+\n",
    "                                                               langs['ja']+langs['vi']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-fr-zh-ja-vi-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-fr-de-no-da-cased  -   num_parameters :  118117870\n",
      "new-models/bert-base-en-fr-de-no-da-cased  -   num_tokens :  41710\n",
      "124.39375185966492\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['de']+\n",
    "                                                               langs['no']+langs['da']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-fr-de-no-da-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-fr-da-ja-vi-cased  -   num_parameters :  119753533\n",
      "new-models/bert-base-en-fr-da-ja-vi-cased  -   num_tokens :  43837\n",
      "106.04906463623047\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['da']+\n",
    "                                                               langs['ja']+langs['vi']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-fr-da-ja-vi-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating trilingual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-fr-es-cased  -   num_parameters :  116154613\n",
      "new-models/bert-base-en-fr-es-cased  -   num_tokens :  39157\n",
      "109.01875948905945\n"
     ]
    }
   ],
   "source": [
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['es']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-fr-es-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-es-it-cased  -   num_parameters :  115747812\n",
      "new-models/bert-base-en-es-it-cased  -   num_tokens :  38628\n",
      "107.14139437675476\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['es']+langs['it']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-es-it-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-es-pt-cased  -   num_parameters :  114788100\n",
      "new-models/bert-base-en-es-pt-cased  -   num_tokens :  37380\n",
      "110.02508020401001\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['es']+langs['pt']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-es-pt-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-fr-de-cased  -   num_parameters :  116043877\n",
      "new-models/bert-base-en-fr-de-cased  -   num_tokens :  39013\n",
      "110.83046197891235\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['de']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-fr-de-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-fr-it-cased  -   num_parameters :  114829626\n",
      "new-models/bert-base-en-fr-it-cased  -   num_tokens :  37434\n",
      "102.47859787940979\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['it']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-fr-it-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-fr-zh-cased  -   num_parameters :  116735208\n",
      "new-models/bert-base-en-fr-zh-cased  -   num_tokens :  39912\n",
      "102.47170114517212\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['zh']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-fr-zh-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-es-zh-cased  -   num_parameters :  118330114\n",
      "new-models/bert-base-en-es-zh-cased  -   num_tokens :  41986\n",
      "121.32015228271484\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['es']+langs['zh']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-es-zh-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-fr-ar-cased  -   num_parameters :  114258259\n",
      "new-models/bert-base-en-fr-ar-cased  -   num_tokens :  36691\n",
      "118.46728754043579\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['fr']+langs['ar']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-fr-ar-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-el-ru-cased  -   num_parameters :  116167686\n",
      "new-models/bert-base-en-el-ru-cased  -   num_tokens :  39174\n",
      "111.97321152687073\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['el']+langs['ru']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-el-ru-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-zh-hi-cased  -   num_parameters :  114350539\n",
      "new-models/bert-base-en-zh-hi-cased  -   num_tokens :  36811\n",
      "84.94034099578857\n"
     ]
    }
   ],
   "source": [
    "del model_cased\n",
    "model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "t = time.time()\n",
    "new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs['zh']+langs['hi']+TOKENS_TO_KEEP)),\n",
    "                             'new-models/bert-base-en-zh-hi-cased')\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating bilingual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-nl-cased  -   num_parameters :  111575987\n",
      "new-models/bert-base-en-nl-cased  -   num_tokens :  33203\n",
      "106.0707049369812\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-uk-cased  -   num_parameters :  113307775\n",
      "new-models/bert-base-en-uk-cased  -   num_tokens :  35455\n",
      "99.78673195838928\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-ro-cased  -   num_parameters :  110857741\n",
      "new-models/bert-base-en-ro-cased  -   num_tokens :  32269\n",
      "92.63383269309998\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-pt-cased  -   num_parameters :  112438805\n",
      "new-models/bert-base-en-pt-cased  -   num_tokens :  34325\n",
      "98.68715047836304\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-it-cased  -   num_parameters :  112105059\n",
      "new-models/bert-base-en-it-cased  -   num_tokens :  33891\n",
      "96.94106602668762\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-lt-cased  -   num_parameters :  110510153\n",
      "new-models/bert-base-en-lt-cased  -   num_tokens :  31817\n",
      "91.33998203277588\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-no-cased  -   num_parameters :  111474479\n",
      "new-models/bert-base-en-no-cased  -   num_tokens :  33071\n",
      "94.8031907081604\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-pl-cased  -   num_parameters :  112090448\n",
      "new-models/bert-base-en-pl-cased  -   num_tokens :  33872\n",
      "97.298259973526\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-da-cased  -   num_parameters :  111213019\n",
      "new-models/bert-base-en-da-cased  -   num_tokens :  32731\n",
      "94.70722985267639\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-en-ja-cased  -   num_parameters :  111672112\n",
      "new-models/bert-base-en-ja-cased  -   num_tokens :  33328\n",
      "105.59650444984436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lang in list(langs.keys())[-10:]:\n",
    "    model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "    t = time.time()\n",
    "    new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs['en']+langs[lang]+TOKENS_TO_KEEP)), \n",
    "                                 'new-models/bert-base-en-'+lang+'-cased')\n",
    "    del model_cased\n",
    "    print(time.time()-t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating monolingual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-nl-cased  -   num_parameters :  104251262\n",
      "new-models/bert-base-nl-cased  -   num_tokens :  23678\n",
      "81.82937741279602\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-uk-cased  -   num_parameters :  95125539\n",
      "new-models/bert-base-uk-cased  -   num_tokens :  11811\n",
      "36.54637026786804\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-ro-cased  -   num_parameters :  102620982\n",
      "new-models/bert-base-ro-cased  -   num_tokens :  21558\n",
      "72.35977983474731\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-pt-cased  -   num_parameters :  105267880\n",
      "new-models/bert-base-pt-cased  -   num_tokens :  25000\n",
      "76.86364984512329\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-it-cased  -   num_parameters :  105649304\n",
      "new-models/bert-base-it-cased  -   num_tokens :  25496\n",
      "81.77199244499207\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-lt-cased  -   num_parameters :  98382254\n",
      "new-models/bert-base-lt-cased  -   num_tokens :  16046\n",
      "48.86039853096008\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-no-cased  -   num_parameters :  104036711\n",
      "new-models/bert-base-no-cased  -   num_tokens :  23399\n",
      "80.93481659889221\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-pl-cased  -   num_parameters :  103266173\n",
      "new-models/bert-base-pl-cased  -   num_tokens :  22397\n",
      "83.10979652404785\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-da-cased  -   num_parameters :  103845230\n",
      "new-models/bert-base-da-cased  -   num_tokens :  23150\n",
      "82.06179618835449\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-models/bert-base-ja-cased  -   num_parameters :  93342228\n",
      "new-models/bert-base-ja-cased  -   num_tokens :  9492\n",
      "32.165956258773804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lang in list(langs.keys())[-10:]:\n",
    "    model_cased = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "    t = time.time()\n",
    "    new_embs = select_embeddings(model_cased, bert_vocab, list(set(langs[lang]+TOKENS_TO_KEEP)), \n",
    "                                 'new-models/bert-base-'+lang+'-cased')\n",
    "    del model_cased\n",
    "    print(time.time()-t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare original and new models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "177974523"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114258259"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new model\n",
    "tokenizer_cust = BertTokenizer.from_pretrained('new-models/bert-base-en-fr-ar-cased')\n",
    "model_cust = BertForMaskedLM.from_pretrained('new-models/bert-base-en-fr-ar-cased')\n",
    "model_cust.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36691"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_cust.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(36691, 768, padding_idx=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cust.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love NLP\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output_original = model(**encoded_input)\n",
    "encoded_input_cust = tokenizer_cust(text, return_tensors='pt')\n",
    "output_cust = model_cust(**encoded_input_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   146, 16138, 81130, 11127,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   11,    54,  3953, 28486,  1043,    12]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input_cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6\n"
     ]
    }
   ],
   "source": [
    "print(len(output_original[0][0]), len(output_cust[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.5026, -8.4598, -8.5441,  ..., -8.4676, -8.3309, -8.4011],\n",
       "        [-7.3152, -7.4170, -7.2602,  ..., -6.7312, -7.1424, -7.0654],\n",
       "        [-8.5618, -9.1271, -7.8516,  ..., -8.3914, -7.0207, -8.6194],\n",
       "        [-6.9560, -6.5613, -6.1853,  ..., -6.7822, -6.2483, -6.3508],\n",
       "        [-8.5762, -7.8520, -6.7847,  ..., -8.3420, -6.3392, -7.7183],\n",
       "        [-7.3157, -7.2129, -6.7588,  ..., -6.8620, -6.5756, -8.0586]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_original[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.5026, -8.4598, -8.5441,  ..., -4.2134, -2.0971, -1.1937],\n",
       "        [-7.3152, -7.4170, -7.2602,  ..., -5.6871, -5.9795, -3.7613],\n",
       "        [-8.5618, -9.1271, -7.8516,  ..., -6.0970, -6.2893,  0.0856],\n",
       "        [-6.9560, -6.5613, -6.1853,  ..., -4.5743, -2.0049, -0.7851],\n",
       "        [-8.5762, -7.8520, -6.7847,  ..., -3.6335, -5.0034, -0.6123],\n",
       "        [-7.3157, -7.2129, -6.7588,  ..., -5.2001, -5.3873,  1.1282]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_cust[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "[-8.502596 -8.459798 -8.544106 -8.420239 -8.55526  -8.383109]\n",
      "[-8.502596 -8.459798 -8.544106 -8.420239 -8.55526  -8.383109]\n",
      "\n",
      "I\n",
      "[-7.3151646 -7.416972  -7.260161  -7.000843  -7.0258822 -6.568579 ]\n",
      "[-7.3151646 -7.416972  -7.260161  -7.000843  -7.0258822 -6.568579 ]\n",
      "\n",
      "love\n",
      "[-8.561801  -9.127073  -7.8515744 -8.405504  -8.349455  -8.195387 ]\n",
      "[-8.561801  -9.127073  -7.8515744 -8.405504  -8.349455  -8.195387 ]\n",
      "\n",
      "NL\n",
      "[-6.9560323 -6.5612655 -6.185295  -5.8626823 -6.8318934 -6.2828846]\n",
      "[-6.9560323 -6.5612655 -6.185295  -5.8626823 -6.8318934 -6.2828846]\n",
      "\n",
      "##P\n",
      "[-8.576168  -7.852015  -6.784669  -7.650504  -7.808926  -7.4190974]\n",
      "[-8.576168  -7.852015  -6.784669  -7.650504  -7.808926  -7.4190974]\n",
      "\n",
      "[SEP]\n",
      "[-7.315665  -7.2128882 -6.75876   -6.995212  -7.240693  -7.080781 ]\n",
      "[-7.315665  -7.2128882 -6.75876   -6.995212  -7.240693  -7.080781 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for input_id in encoded_input['input_ids'][0]:\n",
    "    print(tokenizer.convert_ids_to_tokens(int(input_id)))\n",
    "    print(output_original[0][0][i].detach().numpy()[:6])\n",
    "    print(output_cust[0][0][i].detach().numpy()[:6])\n",
    "    print()\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests on MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital 0.6365790963172913\n",
      "city 0.08376165479421616\n",
      "City 0.034411922097206116\n",
      "port 0.02745007537305355\n",
      "centre 0.012592659331858158\n"
     ]
    }
   ],
   "source": [
    "## declare task ##\n",
    "pipe = pipeline(task=\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "## example ##\n",
    "input_  = 'Paris is the [MASK] of France.'\n",
    "\n",
    "output_ = pipe(input_)\n",
    "for i in range(len(output_)):\n",
    "    print(output_[i]['token_str'], output_[i]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital 0.6015416979789734\n",
      "city 0.07779796421527863\n",
      "City 0.035266101360321045\n",
      "port 0.028833329677581787\n",
      "centre 0.014866690151393414\n"
     ]
    }
   ],
   "source": [
    "## declare task ##\n",
    "pipe = pipeline(task=\"fill-mask\", model=model_cust, tokenizer=tokenizer_cust)\n",
    "\n",
    "## example ##\n",
    "input_  = 'Paris is the [MASK] of France.'\n",
    "\n",
    "output_ = pipe(input_)\n",
    "for i in range(len(output_)):\n",
    "    print(output_[i]['token_str'], output_[i]['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert all models to TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in os.listdir('new-models'):\n",
    "    tf_model = TFBertForMaskedLM.from_pretrained(\"new-models/\"+model_name, from_pt=True)\n",
    "    tf_model.save_pretrained(\"new-models/\"+model_name)\n",
    "    del tf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
